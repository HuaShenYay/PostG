## 目标
- 按 poems.id 顺序分批次（每批 80–100 首）为 `poems.Real_topic` 写入“更接近真实读者评论会提到的主题/意象/情绪标签”。
- 禁止使用 `Bertopic` 等算法字段作为输入；只读 `title/author/content/dynasty/genre_type` 并做数据库写入。

## 写入格式
- `Real_topic` 采用“逗号分隔标签”形式：5–7 个标签。
- 标签结构固定为：
  - 1 个主主题（如：送别/思乡/咏史/边塞/山水/闺怨/怀古/咏物/哲理/爱国等）
  - 1–2 个情绪或评论口吻（如：悲怆/慷慨/凄清/旷达/沉郁/清丽/豪放等）
  - 2–3 个高辨识意象（如：明月/长江/落木/寒蝉/杨柳/梅花/红豆/孤舟等）
  - 0–1 个语境标签（如：怀古/登临/羁旅/悼亡/宴饮/咏志等）

## “真实评论主题”的检索策略（核心）
- 每首诗都执行一次联网检索（优先 2–3 个独立来源交叉验证），把“主旨/意象/情感/常见解读点”抽取为标签。
- 典型检索词模板（按诗种类选择）：
  - `《{title}》 {author} 评论 主题 关键词`
  - `《{title}》 {author} 赏析 主旨 情感 意象`
  - `《{title}》 {author} 为什么` / `名句 解读`
  - 若是词：`《{title}》 {author} 上阕 下阕 情感` / `词牌 赏析`
- 来源优先级（不强制，但尽量覆盖）：
  - 古诗文/古文岛类（主旨、赏析、注释）
  - 百科类（背景、主旨）
  - 高质量专栏/学术文章/教材解读（更明确的“主题词/意象群”）
  - 讨论型平台（知乎等）仅取“高频共识点”，不取情绪化噪音

## 分批工作流（每批 80–100 首）
1. 取数：按 `id` 递增读取下一批 poems（含 title/author/content 等）。
2. 逐首人工判读：
   - 先通读诗（content）确定“讲什么/情绪走向/意象密度”。
   - 再用联网检索确认与补全（尤其是名篇、典故密集、历史背景强的诗）。
   - 生成 5–7 标签，避免泛词（如“诗歌”“好看”）。
3. 批量写回：用一条 `UPDATE ... CASE WHEN id THEN ... END` 写入本批 Real_topic（只改 poems 表）。
4. 校验：
   - 本批范围内 `Real_topic` 非空计数 = 批大小
   - 随机抽样 10–15 首人工复核（看是否偏题/标签重复/意象不贴合）

## 质量控制与纠错
- 若检索结果彼此冲突：以“内容直读 + 权威赏析/教材解读”为准。
- 对高度同质的词作（大量“离愁别绪/春愁秋恨”）：强制加入 1–2 个更具体意象或情境（如“清明/寒食/长亭/雁/西湖/江南”等），避免全部写成同一套标签。
- 对冷门作品检索不到：仍按内容直读给出主题，但会优先用“同作者/同题材/同意象”的检索补证。

## 进度与顺序
- 从当前尚未完成的最小 id 开始，连续分批推进直到 850 首全部覆盖。
- 每批结束输出：本批 id 范围、写入数量、抽样复核示例。

## 风险与约束
- 互联网检索能覆盖名篇与常见作品，但对极冷门作品可能缺少“真实评论”数据；此类将以“多来源赏析共识 + 逐首直读”替代，保证主题尽可能客观。

如果你确认这个计划，我将退出 plan mode 并从下一批（按 id 连续）开始执行 80–100 首的写入与回查。