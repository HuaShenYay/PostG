#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€é”®é‡æ–°è®­ç»ƒæ‰€æœ‰ç”¨æˆ·LDAæ¨¡å‹çš„è„šæœ¬

åŠŸèƒ½ï¼š
1. é‡æ–°è®­ç»ƒLDAæ¨¡å‹ï¼ˆåŸºäºæ‰€æœ‰è¯„è®ºæ•°æ®ï¼‰
2. ä¸ºæ‰€æœ‰è¯„è®ºé‡æ–°æ¨æ–­ä¸»é¢˜åˆ†å¸ƒ
3. æ›´æ–°æ‰€æœ‰ç”¨æˆ·çš„åå¥½ç”»åƒ
4. ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°

ä½¿ç”¨æ–¹æ³•ï¼š
    python retrain_all_users_lda.py

ä½œè€…ï¼šè¯—äº‘å›¢é˜Ÿ
æ—¥æœŸï¼š2024
"""

import sys
import os
import time
import pandas as pd

# æ·»åŠ åç«¯è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import text
from datetime import datetime
from config import Config
from models import db, User, Poem, Review
from lda_analysis import (
    train_lda_model, 
    load_stopwords, 
    preprocess_text, 
    preprocess_text_advanced,
    filter_by_frequency,
    save_lda_model
)
import json
from collections import Counter


def create_app():
    """åˆ›å»ºFlaskåº”ç”¨å®ä¾‹"""
    app = Flask(__name__)
    app.config.from_object(Config)
    db.init_app(app)
    return app


def get_all_reviews_df(app):
    """è·å–å…¨é‡è¯„è®ºçš„DataFrame"""
    with app.app_context():
        sql = text("""
            SELECT r.id as review_id, r.user_id, r.poem_id, r.comment, r.topic_distribution
            FROM reviews r
        """)
        try:
            return pd.read_sql(sql, db.session.connection())
        except Exception:
            return pd.read_sql(sql, db.engine)


def update_user_preference_batch(app, user_ids):
    """æ‰¹é‡æ›´æ–°ç”¨æˆ·åå¥½ç”»åƒ"""
    with app.app_context():
        updated_count = 0
        
        for user_id in user_ids:
            reviews = Review.query.filter(
                Review.user_id == user_id, 
                Review.topic_distribution != None
            ).all()
            
            if not reviews:
                continue
            
            # èšåˆç”¨æˆ·çš„æ‰€æœ‰è¯„è®ºä¸»é¢˜åˆ†å¸ƒ
            user_dist = {}
            for r in reviews:
                dist = json.loads(r.topic_distribution)
                for tid, prob in dist.items():
                    user_dist[tid] = user_dist.get(tid, 0) + prob
            
            # å½’ä¸€åŒ–
            total = sum(user_dist.values()) or 1
            preference = []
            for tid, score in user_dist.items():
                preference.append({
                    "topic_id": int(tid),
                    "score": float(score / total)
                })
            
            # æŒ‰å¾—åˆ†æ’åº
            preference.sort(key=lambda x: x['score'], reverse=True)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            user = User.query.get(user_id)
            if user:
                user.preference_topics = json.dumps(preference)
                db.session.commit()
                updated_count += 1
                print(f"  âœ“ ç”¨æˆ· {user.username} (ID:{user_id}) åå¥½å·²æ›´æ–°")
        
        return updated_count


def retrain_all_users_lda():
    """
    ä¸€é”®é‡æ–°è®­ç»ƒæ‰€æœ‰ç”¨æˆ·LDAæ¨¡å‹
    
    æµç¨‹ï¼š
    1. åŠ è½½æ•°æ®åº“æ•°æ®
    2. é‡æ–°è®­ç»ƒLDAæ¨¡å‹
    3. ä¸ºæ‰€æœ‰è¯„è®ºé‡æ–°æ¨æ–­ä¸»é¢˜åˆ†å¸ƒ
    4. æ›´æ–°æ‰€æœ‰ç”¨æˆ·åå¥½ç”»åƒ
    5. ä¿å­˜æ¨¡å‹
    """
    print("=" * 60)
    print("ğŸš€ å¼€å§‹ä¸€é”®é‡æ–°è®­ç»ƒæ‰€æœ‰ç”¨æˆ·LDAæ¨¡å‹")
    print("=" * 60)
    print()
    
    start_time = time.time()
    
    # 1. åˆ›å»ºåº”ç”¨å¹¶è¿æ¥æ•°æ®åº“
    print("ğŸ“¦ æ­¥éª¤1: è¿æ¥æ•°æ®åº“...")
    app = create_app()
    
    with app.app_context():
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            db.engine.connect()
            print("  âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    # 2. è·å–å…¨é‡è¯„è®ºæ•°æ®
    print("\nğŸ“š æ­¥éª¤2: åŠ è½½å…¨é‡è¯„è®ºæ•°æ®...")
    df_reviews = get_all_reviews_df(app)
    total_reviews = len(df_reviews)
    total_users = df_reviews['user_id'].nunique() if total_reviews > 0 else 0
    
    print(f"  âœ… åŠ è½½äº† {total_reviews} æ¡è¯„è®º")
    print(f"  âœ… æ¶‰åŠ {total_users} ä½ç”¨æˆ·")
    
    if total_reviews == 0:
        print("  âš ï¸  æ²¡æœ‰è¯„è®ºæ•°æ®ï¼Œæ— éœ€è®­ç»ƒ")
        return True
    
    # 3. é‡æ–°è®­ç»ƒLDAæ¨¡å‹
    print("\nğŸ¯ æ­¥éª¤3: é‡æ–°è®­ç»ƒLDAæ¨¡å‹...")
    print("  â³ è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # åŠ è½½åœç”¨è¯
    stopwords = load_stopwords()
    print(f"  ğŸ“– åŠ è½½äº† {len(stopwords)} ä¸ªåœç”¨è¯")
    
    # é¢„å¤„ç†è¯„è®º
    print("  âœ‚ï¸ æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†...")
    tokenized_texts = df_reviews['comment'].apply(
        lambda x: preprocess_text(str(x), stopwords)
    ).tolist()
    
    # è¯é¢‘è¿‡æ»¤
    tokenized_texts, valid_words = filter_by_frequency(
        tokenized_texts, 
        min_freq=2,
        max_doc_ratio=0.8
    )
    
    # ä½¿ç”¨é«˜çº§é¢„å¤„ç†é‡æ–°å¤„ç†
    tokenized_texts = [
        preprocess_text_advanced(str(text), stopwords, valid_words)
        for text in df_reviews['comment']
    ]
    
    print(f"  âœ… å®Œæˆé¢„å¤„ç†ï¼Œå…± {len(tokenized_texts)} æ¡æ–‡æœ¬")
    
    # è®­ç»ƒLDAæ¨¡å‹
    lda, dictionary, df, topic_keywords = train_lda_model(
        df_reviews, 
        use_advanced_preprocessing=False  # å·²ç»é¢„å¤„ç†è¿‡äº†
    )
    
    if lda is None:
        print("  âŒ LDAæ¨¡å‹è®­ç»ƒå¤±è´¥")
        return False
    
    print(f"  âœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆ")
    print(f"     - ä¸»é¢˜æ•°: {len(topic_keywords)}")
    print(f"     - è¯æ±‡è¡¨å¤§å°: {len(dictionary)}")
    
    # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯
    print("\nğŸ“‹ ä¸»é¢˜å…³é”®è¯é¢„è§ˆ:")
    for topic_id, keywords in topic_keywords.items():
        print(f"  ä¸»é¢˜ {topic_id}: {', '.join(keywords[:5])}")
    
    # 4. ä¸ºæ‰€æœ‰è¯„è®ºæ¨æ–­ä¸»é¢˜åˆ†å¸ƒ
    print("\nğŸ”„ æ­¥éª¤4: ä¸ºæ‰€æœ‰è¯„è®ºé‡æ–°æ¨æ–­ä¸»é¢˜åˆ†å¸ƒ...")
    
    with app.app_context():
        all_reviews = Review.query.all()
        processed_count = 0
        
        for r in all_reviews:
            tokens = preprocess_text(str(r.comment), stopwords)
            
            if tokens:
                bow = dictionary.doc2bow(tokens)
                dist = dict(lda[bow])
                r.topic_distribution = json.dumps({str(k): float(v) for k, v in dist.items()})
            else:
                r.topic_distribution = json.dumps({})
            
            processed_count += 1
        
        db.session.commit()
        print(f"  âœ… å·²æ›´æ–° {processed_count}/{len(all_reviews)} æ¡è¯„è®ºçš„ä¸»é¢˜åˆ†å¸ƒ")
    
    # 5. æ›´æ–°æ‰€æœ‰ç”¨æˆ·åå¥½ç”»åƒ
    print("\nğŸ‘¤ æ­¥éª¤5: æ›´æ–°æ‰€æœ‰ç”¨æˆ·åå¥½ç”»åƒ...")
    
    with app.app_context():
        all_user_ids = [u.id for u in User.query.all()]
        updated_users = update_user_preference_batch(app, all_user_ids)
        print(f"  âœ… å·²æ›´æ–° {updated_users}/{len(all_user_ids)} ä½ç”¨æˆ·çš„åå¥½ç”»åƒ")
    
    # 6. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ æ­¥éª¤6: ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°...")
    
    try:
        save_lda_model(lda, dictionary, topic_keywords)
        print("  âœ… æ¨¡å‹å·²ä¿å­˜åˆ° saved_models/ ç›®å½•")
    except Exception as e:
        print(f"  âš ï¸  æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    # 7. å®Œæˆ
    elapsed_time = time.time() - start_time
    
    print()
    print("=" * 60)
    print("âœ… ä¸€é”®é‡æ–°è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print()
    print("ğŸ“Š æ‰§è¡Œæ‘˜è¦:")
    print(f"   - å¤„ç†è¯„è®ºæ•°: {total_reviews}")
    print(f"   - æ¶‰åŠç”¨æˆ·æ•°: {total_users}")
    print(f"   - LDAä¸»é¢˜æ•°: {len(topic_keywords)}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {len(dictionary)}")
    print(f"   - æ›´æ–°ç”¨æˆ·æ•°: {updated_users}")
    print(f"   - æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print()
    print("ğŸ’¡ æç¤º: æ¨¡å‹å·²è‡ªåŠ¨é‡æ–°åŠ è½½ï¼Œä¸‹æ¬¡APIè°ƒç”¨å°†ä½¿ç”¨æ–°æ¨¡å‹")
    print()
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    try:
        retrain_all_users_lda()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
